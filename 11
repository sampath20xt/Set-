import os
import time
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain_groq import ChatGroq
from langchain.schema import Document
from dotenv import load_dotenv, find_dotenv
from tqdm import tqdm  # For progress tracking

# Load environment variables
load_dotenv(find_dotenv())

# Set API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Load CSV Data
csv_file_path = "Data/inventory.csv"
df = pd.read_csv(csv_file_path)

# **🔹 Smart Chunking: Group 20 Rows Together**
def group_rows_into_chunks(df, rows_per_chunk=20):
    """Groups multiple rows into structured text chunks."""
    text_chunks = []
    for i in range(0, len(df), rows_per_chunk):
        chunk_df = df.iloc[i:i + rows_per_chunk]  # Select a batch of rows
        chunk_text = "\n".join(chunk_df.astype(str).apply(lambda row: " | ".join(row), axis=1))  # Convert to text
        text_chunks.append(Document(page_content=chunk_text))
    return text_chunks

# Generate structured chunks (each with 20 rows)
chunks = group_rows_into_chunks(df, rows_per_chunk=20)

# Print chunking info
print(f"✅ Total chunks generated: {len(chunks)} (Each chunk contains 20 rows)")

# Initialize Google Gemini Embeddings
embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)

# Create or connect to Pinecone index
index_name = "rag-index"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=768,  # Match Gemini embedding output
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
    print(f"🕒 Waiting for index {index_name} to be ready...")
    time.sleep(10)  # Wait for index readiness

# Connect to the Pinecone index
index = pc.Index(index_name)

# **🔍 Debug: Check if embeddings were already stored**
index_stats = index.describe_index_stats()
print(f"\n🔹 Pinecone Index Stats: {index_stats}")

# **Store embeddings if not already stored**
if index_stats["total_vector_count"] == 0:
    print("\n🚀 Generating embeddings and storing in Pinecone...")
    
    vectors = []
    for i, doc in tqdm(enumerate(chunks), total=len(chunks), desc="Processing Chunks"):
        print(f"📝 Generating embedding for chunk {i+1}/{len(chunks)}...")
        embedding = embedding_model.embed_query(doc.page_content)  # Generate embeddings
        vectors.append((f"doc_{i}", embedding, {"text": doc.page_content}))  # Store text as metadata
    
    print(f"\n⬆️ Uploading {len(vectors)} embeddings to Pinecone...")
    index.upsert(vectors=vectors)  # Upload all embeddings at once
    print("✅ Embeddings successfully stored in Pinecone.")

else:
    print("✅ Embeddings already exist in Pinecone. Skipping storage.")

# Load Chat Model (Groq Llama-3.3-70B)
chat_model = ChatGroq(model_name="llama-3.3-70b-versatile", api_key=GROQ_API_KEY)

def reformulate_query(user_query):
    """Use LLM to convert user question into structured query format."""
    print(f"\n🔍 Reformulating user query: {user_query}")

    prompt = f"""
    You are a search assistant. Convert the following user question into a structured search query 
    that matches a tabular inventory database with fields like 'Product', 'Warehouse', 'Quantity', 'Inventory Age'.

    User Query: "{user_query}"

    Expected output format: 'Product | Warehouse | Quantity | Inventory Age'
    """

    structured_query = chat_model.invoke(prompt).content
    print(f"✅ Reformulated Query: {structured_query}")
    return structured_query

def retrieve_and_generate_response(user_query):
    """Retrieve relevant documents and generate a response with debugging."""
    print(f"\n🔍 Received User Query: {user_query}")

    # **🔹 Step 1: Convert User Query to Structured Query**
    structured_query = reformulate_query(user_query)
    
    # **🔹 Step 2: Generate embedding for the structured query**
    query_embedding = embedding_model.embed_query(structured_query)

    print("🔍 Searching Pinecone for relevant chunks...")
    search_results = index.query(vector=query_embedding, top_k=10, include_metadata=True)

    # **🔹 Step 3: Debug - Print retrieved results**
    print("\n🔹 Search Results (Top 5 Matches):")
    if not search_results["matches"]:
        print("⚠️ No relevant information found. Try rephrasing the query.")
        return "⚠️ No relevant information found. Try rephrasing the query."

    for match in search_results["matches"]:
        print(f" - Score: {match['score']}, Text: {match['metadata']['text'][:200]}...")

    # Retrieve relevant text from metadata
    retrieved_texts = [match["metadata"]["text"] for match in search_results["matches"]]
    retrieved_text = "\n\n".join(retrieved_texts)

    print("\n💡 Generating response from Llama-3.3-70B...")
    response = chat_model.invoke(
        f"Based on the following relevant information, answer the query:\n\n{retrieved_text}\n\nQuery: {user_query}"
    )

    return response.content

# **🔹 Example User Query**
user_input = "What are the product names for warehouse Riverside?"
response = retrieve_and_generate_response(user_input)
print("\n🤖 AI Response:", response)
