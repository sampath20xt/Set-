import os
import pandas as pd
from operator import index
from langchain_groq import ChatGroq
from dotenv import load_dotenv, find_dotenv
from langchain_community.retrievers import PineconeHybridSearchRetriever
from pinecone_text.sparse import BM25Encoder
from pinecone import Pinecone, ServerlessSpec
from langchain_huggingface import HuggingFaceEmbeddings
from nltk.tokenize import sent_tokenize

# Load environment variables
load_dotenv(find_dotenv())

# MongoDB & API Keys
MONGO_URI = os.getenv("MONGO_URI")
MONGO_DB = "Superset"
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
MONGO_COLLECTION = "csv_embeddings"
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

llm_model = ChatGroq(model="llama-3.3-70b-versatile")
index_name = "hybrid-search-langchain-pinecone"

pc = Pinecone(PINECONE_API_KEY=PINECONE_API_KEY)

# Ensure the index exists
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric='dotproduct',
        spec=ServerlessSpec(cloud='aws', region="us-east-1"),
    )

# Retrieve the actual Pinecone index object
index = pc.Index(index_name)

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Modify this line to include your CSV file path
csv_file_path = "Data/inventory.csv"  # Replace with actual CSV file path

BM25_Encoder = BM25Encoder().default()

# Read CSV in chunks
chunksize = 500  # Adjust as needed for large files

def chunk_text(text, max_chunk_size=512):
    """Splits text into chunks of max_chunk_size tokens."""
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_length = len(sentence.split())  # Count words/tokens
        if current_length + sentence_length > max_chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0
        current_chunk.append(sentence)
        current_length += sentence_length

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# Process CSV in chunks
for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):
    for _, row in chunk.iterrows():
        text_data = row.to_string(index=False)  # Convert row to text
        text_chunks = chunk_text(text_data)  # Chunk the text
        for chunk_text in text_chunks:
            BM25_Encoder.fit(chunk_text)

BM25_Encoder.dump("BM25_values.json")
BM25_Encoder = BM25Encoder().load("BM25_values.json")

# Pass the Pinecone index object instead of a string
retriever = PineconeHybridSearchRetriever(embeddings=embeddings, sparse_encoder=BM25_Encoder, index=index)

# User input for query
user_query = input("Enter your search query: ")

# Retrieve relevant documents based on the user's query
results = retriever.invoke(user_query)

# Print the search results
print(results)
