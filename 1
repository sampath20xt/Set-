from operator import index
from langchain_groq import ChatGroq
from dotenv import load_dotenv, find_dotenv
from langchain_community.retrievers import PineconeHybridSearchRetriever
import os
from pinecone_text.sparse import BM25Encoder
from pinecone import Pinecone, ServerlessSpec
from langchain_huggingface import HuggingFaceEmbeddings

from demo7 import retriever  # Removed user_query, as we will take input dynamically

# Load environment variables
load_dotenv(find_dotenv())

# MongoDB & API Keys
MONGO_URI = os.getenv("MONGO_URI")
MONGO_DB = "Superset"
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
MONGO_COLLECTION = "csv_embeddings"
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

llm_model = ChatGroq(model="llama-3.3-70b-versatile")
index_name = "hybrid_search_langchain_pinecone"

pc = Pinecone(PINECONE_API_KEY=PINECONE_API_KEY)

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric='dotproduct',
        spec=ServerlessSpec(cloud='aws', region="us-east-1"),
    )

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Modify this line to include your CSV file path
csv_file_path = "your_file.csv"  # Replace with actual CSV file path

BM25_Encoder = BM25Encoder().default()
BM25_Encoder.fit(csv_file_path)
BM25_Encoder.dump("BM25_values.json")
BM25_Encoder = BM25Encoder().load("BM25_values.json")

retriever = PineconeHybridSearchRetriever(embeddings=embeddings, sparse_encoder=BM25_Encoder, index=index_name)

# User input for query
user_query = input("Enter your search query: ")

# Retrieve relevant documents based on the user's query
results = retriever.invoke(user_query)

# Print the search results
print(results)
