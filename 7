import os
import time
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain_groq import ChatGroq
from langchain.schema import Document
from dotenv import load_dotenv, find_dotenv

# Load environment variables
load_dotenv(find_dotenv())

# Set API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Load CSV Data
csv_file_path = "Data/inventory.csv"
df = pd.read_csv(csv_file_path)

# Combine all text into a single column if needed
text_data = df.apply(lambda row: " ".join(row.astype(str)), axis=1).tolist()

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
documents = [Document(page_content=text) for text in text_data]
chunks = text_splitter.split_documents(documents)

# Initialize Google Gemini Embeddings
embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)

# Create or connect to Pinecone index
index_name = "rag-index"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=768,  # Adjust to match Gemini output
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")  # Adjust region if needed
    )
    print(f"Waiting for index {index_name} to be ready...")
    time.sleep(10)  # Wait for the index to be ready

# Connect to the Pinecone index
index = pc.Index(index_name)

# Store embeddings in Pinecone (if not already stored)
if index.describe_index_stats()["total_vector_count"] == 0:
    print("Storing embeddings in Pinecone...")
    vectors = []
    for i, doc in enumerate(chunks):
        embedding = embedding_model.embed_query(doc.page_content)  # Generate embeddings
        vectors.append((f"doc_{i}", embedding, {"text": doc.page_content}))  # Store text as metadata
    
    index.upsert(vectors=vectors)  # Upload all embeddings at once
    print(f"Stored {len(vectors)} embeddings in Pinecone.")

# Load Chat Model (Groq Llama-3.3-70B)
chat_model = ChatGroq(model_name="llama-3.3-70b-versatile", api_key=GROQ_API_KEY)


def retrieve_and_generate_response(user_query):
    """Retrieve relevant documents and generate a response."""
    # Generate query embedding
    query_embedding = embedding_model.embed_query(user_query)

    # Search Pinecone for relevant chunks
    search_results = index.query(vector=query_embedding, top_k=5, include_metadata=True)

    # Retrieve relevant text from metadata
    retrieved_texts = [match["metadata"]["text"] for match in search_results["matches"]]

    if not retrieved_texts:
        return "No relevant information found."

    retrieved_text = "\n\n".join(retrieved_texts)

    # Generate response using Groq AI
    response = chat_model.invoke(
        f"Based on the following relevant information, answer the query:\n\n{retrieved_text}\n\nQuery: {user_query}"
    )

    return response.content


# Example Query
user_input = "What are the product names for warehouse Riverside?"
response = retrieve_and_generate_response(user_input)
print(response)
